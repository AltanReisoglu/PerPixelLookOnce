# ğŸ§  PerPixelLookOnce

**PerPixelLookOnce** is a prompt-based object detection system that identifies and highlights only the objects specified in a natural language query. The model architecture is based on a **YOLO-style detection backbone**, enhanced with **Transformer-based self-attention mechanisms** for improved contextual understanding and accuracy.

## ğŸš€ Key Features

- ğŸ—£ï¸ **Prompt-guided detection**: Detects objects only if they match the user's prompt (e.g., "bicycle", "person").
- âš¡ **YOLO-inspired fast inference**: Uses a single-shot detection strategy for high-speed processing.
- ğŸ§  **Transformer-enhanced**: Integrates self-attention modules to better capture spatial and semantic relationships.
- ğŸ–¼ï¸ **Per-pixel decision making**: Each pixel is processed once, reducing redundancy while preserving detail.
- ğŸ¯ **Selective bounding box generation**: Only draws boxes around prompt-relevant objects.

## ğŸ“ Project Structure

```bash
PerPixelLookOnce/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model.py           # YOLOv3 based + Self-Attention +Axial Attention architecture
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample.jpg         # Sample image for testing
â”œâ”€â”€ detect.py              # Inference script
â”œâ”€â”€ utils.py               # Utility functions (e.g., drawing boxes)
â”œâ”€â”€ config.yaml            # Model config
â”œâ”€â”€ requirements.txt       # Required libraries
â””â”€â”€ README.md              # Project documentation

## Usage
python detect.py --image ./data/sample.jpg --prompt "car"
